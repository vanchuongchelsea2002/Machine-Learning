
import streamlit as st
import os
#import cv2
import numpy as np
import pandas as pd
import pickle
import seaborn as sns
import random
import struct
import time
import altair
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA
import mlflow
from sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay,adjusted_rand_score
from PIL import Image
from collections import Counter
from mlflow.tracking import MlflowClient
def run_ClusteringMinst_app():
    @st.cache_data  # L∆∞u cache ƒë·ªÉ tr√°nh load l·∫°i d·ªØ li·ªáu m·ªói l·∫ßn ch·∫°y l·∫°i Streamlit
    def get_sampled_pixels(images, sample_size=100_000):
        return np.random.choice(images.flatten(), sample_size, replace=False)

    @st.cache_data  # Cache danh s√°ch ·∫£nh ng·∫´u nhi√™n
    def get_random_indices(num_images, total_images):
        return np.random.randint(0, total_images, size=num_images)

    # C·∫•u h√¨nh Streamlit
    # st.set_page_config(page_title="Ph√¢n lo·∫°i ·∫£nh", layout="wide")
    # ƒê·ªãnh nghƒ©a h√†m ƒë·ªÉ ƒë·ªçc file .idx
    def load_mnist_images(filename):
        with open(filename, 'rb') as f:
            magic, num, rows, cols = struct.unpack('>IIII', f.read(16))
            images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)
        return images

    def load_mnist_labels(filename):
        with open(filename, 'rb') as f:
            magic, num = struct.unpack('>II', f.read(8))
            labels = np.fromfile(f, dtype=np.uint8)
        return labels

    mlflow_tracking_uri = st.secrets["MLFLOW_TRACKING_URI"]
    mlflow_username = st.secrets["MLFLOW_TRACKING_USERNAME"]
    mlflow_password = st.secrets["MLFLOW_TRACKING_PASSWORD"]
    
    # Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng
    os.environ["MLFLOW_TRACKING_URI"] = mlflow_tracking_uri
    os.environ["MLFLOW_TRACKING_USERNAME"] = mlflow_username
    os.environ["MLFLOW_TRACKING_PASSWORD"] = mlflow_password
    
    # Thi·∫øt l·∫≠p MLflow (ƒê·∫∑t sau khi mlflow_tracking_uri ƒë√£ c√≥ gi√° tr·ªã)
    mlflow.set_tracking_uri(mlflow_tracking_uri)



    # ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file MNIST
    # dataset_path = r"C:\Users\Dell\OneDrive\Pictures\Documents\Code\python\OpenCV\HMVPYTHON\App"
    dataset_path = os.path.dirname(os.path.abspath(__file__)) 
    train_images_path = os.path.join(dataset_path, "train-images.idx3-ubyte")
    train_labels_path = os.path.join(dataset_path, "train-labels.idx1-ubyte")
    test_images_path = os.path.join(dataset_path, "t10k-images.idx3-ubyte")
    test_labels_path = os.path.join(dataset_path, "t10k-labels.idx1-ubyte")

    if "train_images" not in st.session_state:
            st.session_state.train_images = load_mnist_images(train_images_path)
            st.session_state.train_labels = load_mnist_labels(train_labels_path)
            st.session_state.test_images = load_mnist_images(test_images_path)
            st.session_state.test_labels = load_mnist_labels(test_labels_path)
    # T·∫£i d·ªØ li·ªáu
    train_images = load_mnist_images(train_images_path)
    train_labels = load_mnist_labels(train_labels_path)
    test_images = load_mnist_images(test_images_path)
    test_labels = load_mnist_labels(test_labels_path)

    

    # Giao di·ªán Streamlit
    st.title("üì∏ MNIST Clustering")
    tabs = st.tabs([
            "Th√¥ng tin",
            "T·∫≠p d·ªØ li·ªáu",
            "X·ª≠ l√≠ d·ªØ li·ªáu",
            "Ph√¢n c·ª•m d·ªØ li·ªáu",
            "D·ª± ƒëo√°n",
            "Th√¥ng tin & Mlflow",
    ])
    tab_note,tab_info,tab_load, tab_preprocess, tab_demo ,tab_mlflow= tabs


    with tab_note:
        with st.expander("**Th√¥ng tin m√¥ h√¨nh**", expanded=True):
            # Ch·ªçn m√¥ h√¨nh
            model_option1 = st.selectbox("Ch·ªçn m√¥ h√¨nh", ["K-Means", "DBSCAN"])
            
            if model_option1 == "K-Means":
                st.markdown("## üîπ K-Means Clustering")
                st.markdown("---")

                st.markdown("**Kh√°i ni·ªám**")
                st.write("""
                - **K-Means** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m kh√¥ng gi√°m s√°t, chia t·∫≠p d·ªØ li·ªáu th√†nh $K$ c·ª•m (clusters) sao cho c√°c ƒëi·ªÉm trong c√πng m·ªôt c·ª•m g·∫ßn nhau nh·∫•t, d·ª±a tr√™n kho·∫£ng c√°ch (th∆∞·ªùng l√† kho·∫£ng c√°ch Euclidean) ƒë·∫øn t√¢m c·ª•m (centroid).
                - M·ª•c ti√™u c·ªßa K-Means l√† **t·ªëi thi·ªÉu h√≥a t·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch** (Within-Cluster Sum of Squares - WCSS) gi·ªØa c√°c ƒëi·ªÉm d·ªØ li·ªáu v√† t√¢m c·ª•m t∆∞∆°ng ·ª©ng c·ªßa ch√∫ng.
                """)

                st.markdown("""
                ### üîÑ **Quy tr√¨nh ho·∫°t ƒë·ªông c·ªßa K-Means**

                - **B∆∞·ªõc 1**: Kh·ªüi t·∫°o ng·∫´u nhi√™n $K$ t√¢m c·ª•m $\mu_1, \mu_2, ..., \mu_K$.

                - **B∆∞·ªõc 2**: L·∫∑p l·∫°i qu√° tr√¨nh c·∫≠p nh·∫≠t t√¢m c·ª•m cho t·ªõi khi d·ª´ng:
                - **a**. X√°c ƒë·ªãnh nh√£n cho t·ª´ng ƒëi·ªÉm d·ªØ li·ªáu $c_i$ d·ª±a v√†o kho·∫£ng c√°ch t·ªõi t·ª´ng t√¢m c·ª•m:
                    $$
                    c_i = arg\min_j \|x_i - \mu_j\|^2
                    $$
                **Trong ƒë√≥**:  
                - $$c_i$$ l√† ch·ªâ s·ªë c·ª•m (t·ª´ 1 ƒë·∫øn $K$).  
                - $$\|x_i - \mu_j\|^2$$ l√† kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng gi·ªØa ƒëi·ªÉm $x_i$ v√† t√¢m c·ª•m $\mu_j$.

                - **b**. T√≠nh l·∫°i t√¢m c·ª•m $\mu_j$ b·∫±ng trung b√¨nh c·ªßa t·∫•t c·∫£ c√°c ƒëi·ªÉm d·ªØ li·ªáu thu·ªôc c·ª•m $j$:
                """)

                st.latex(r"\mu_j = \frac{\sum_{i=1}^{n} I(c_i = j) x_i}{\sum_{i=1}^{n} I(c_i = j)}")

                st.markdown("""
                **Trong ƒë√≥**: 
                - Gi√° tr·ªã c·ªßa $$(I(c_i = j))$$ ph·ª• thu·ªôc v√†o ƒëi·ªÅu ki·ªán $$(c_i = j)$$:
                    - N·∫øu $$(c_i = j)$$ (t·ª©c l√† ƒëi·ªÉm d·ªØ li·ªáu th·ª© $$i$$ thu·ªôc c·ª•m $$j$$), th√¨ $$(I(c_i = j))$$ = 1.
                    - N·∫øu $$(c_i ‚â† j)$$ (t·ª©c l√† ƒëi·ªÉm d·ªØ li·ªáu th·ª© $$i$$ kh√¥ng thu·ªôc c·ª•m $$j$$), th√¨ $$(I(c_i = j))$$ = 0.
                - $$(n)$$ l√† s·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu.
                - Thu·∫≠t to√°n d·ª´ng khi t√¢m c·ª•m kh√¥ng thay ƒë·ªïi gi·ªØa c√°c v√≤ng l·∫∑p ho·∫∑c ƒë·∫°t s·ªë l·∫ßn l·∫∑p t·ªëi ƒëa (`max_iter`).
                """)
                st.markdown("---")
                st.markdown("### üìê **C√¥ng th·ª©c to√°n h·ªçc**")
                st.write("""
                - M·ª•c ti√™u t·ªëi ∆∞u h√≥a c·ªßa K-Means l√†:  
                $$
                J = \sum_{k=1}^{K}\sum_{i=1}^{n} ||x_i - \mu_k||^2
                $$
                Trong ƒë√≥:  
                - \(J\): T·ªïng b√¨nh ph∆∞∆°ng kho·∫£ng c√°ch (WCSS - Within-Cluster Sum of Squares).  
                - \(n\): S·ªë l∆∞·ª£ng ƒëi·ªÉm d·ªØ li·ªáu.  
                - \(K\): S·ªë c·ª•m.  
                - $$(x_i)$$: ƒêi·ªÉm d·ªØ li·ªáu th·ª© \(i\).  
                - $$(\mu_k)$$: T√¢m c·ª•m c·ªßa c·ª•m \(k\).  
                - $$(||x_i - \mu_k||^2)$$: Kho·∫£ng c√°ch Euclidean b√¨nh ph∆∞∆°ng gi·ªØa ƒëi·ªÉm $$(x_i)$$ v√† t√¢m c·ª•m $$(\mu_k)$$.
                """)
                st.markdown("---")
                st.markdown("### üîß **M·ªôt s·ªë c·∫£i ti·∫øn c·ªßa K-Means**")
                st.write("""
                - **Mini-Batch K-Means**: S·ª≠ d·ª•ng c√°c batch nh·ªè c·ªßa d·ªØ li·ªáu ƒë·ªÉ c·∫≠p nh·∫≠t t√¢m c·ª•m, gi√∫p gi·∫£m th·ªùi gian t√≠nh to√°n tr√™n d·ªØ li·ªáu l·ªõn.
                - **K-Means v·ªõi chu·∫©n h√≥a d·ªØ li·ªáu**: Chu·∫©n h√≥a (scaling) d·ªØ li·ªáu tr∆∞·ªõc khi √°p d·ª•ng K-Means ƒë·ªÉ tr√°nh ƒë·∫∑c tr∆∞ng c√≥ thang ƒëo l·ªõn ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ ph√¢n c·ª•m.
                """)
                

                means = [[2, 2], [8, 3], [3, 6]]
                cov = [[1, 0], [0, 1]]
                N = 500

                # T·∫°o d·ªØ li·ªáu ph√¢n c·ª•m
                X0 = np.random.multivariate_normal(means[0], cov, N)
                X1 = np.random.multivariate_normal(means[1], cov, N)
                X2 = np.random.multivariate_normal(means[2], cov, N)

                X = np.concatenate((X0, X1, X2), axis=0)
                K = 3

                # T·∫°o nh√£n ban ƒë·∫ßu
                original_label = np.asarray([0]*N + [1]*N + [2]*N)  # Lo·∫°i b·ªè .T v√¨ kh√¥ng c·∫ßn thi·∫øt

                # H√†m hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m, tr·∫£ v·ªÅ figure ƒë·ªÉ s·ª≠ d·ª•ng b√™n ngo√†i
                def kmeans_display(X, labels):
                    K = np.amax(labels) + 1  # S·ªë c·ª•m (K)
                    
                    # T·∫°o figure v√† axes cho matplotlib
                    fig, ax = plt.subplots(figsize=(8, 6))
                    
                    # V·∫Ω t·ª´ng c·ª•m v·ªõi m√†u v√† ki·ªÉu kh√°c nhau
                    colors = ['blue', 'green', 'red']  # M√†u cho 3 c·ª•m
                    markers = ['^', 'o', 's']  # Ki·ªÉu marker cho 3 c·ª•m
                    
                    for k in range(K):
                        X_k = X[labels == k, :]
                        ax.plot(X_k[:, 0], X_k[:, 1], 
                                marker=markers[k], 
                                color=colors[k], 
                                markersize=4, 
                                alpha=0.8, 
                                label=f'C·ª•m {k}')
                    
                    # Thi·∫øt l·∫≠p giao di·ªán bi·ªÉu ƒë·ªì
                    ax.set_xlabel("T·ªça ƒë·ªô X")
                    ax.set_ylabel("T·ªça ƒë·ªô Y")
                    ax.set_title("Bi·ªÉu ƒë·ªì ph√¢n c·ª•m K-Means v·ªõi d·ªØ li·ªáu c·ªë ƒë·ªãnh")
                    ax.legend()
                    ax.axis('equal')  # ƒê·∫£m b·∫£o t·ª∑ l·ªá tr·ª•c x v√† y b·∫±ng nhau
                    
                    return fig  # Tr·∫£ v·ªÅ figure ƒë·ªÉ s·ª≠ d·ª•ng b√™n ngo√†i

                # Giao di·ªán Streamlit
                st.markdown("---")
                st.markdown("### üìä **Ph√¢n c·ª•m K-Means v·ªõi d·ªØ li·ªáu c·ªë ƒë·ªãnh**")

                # Hi·ªÉn th·ªã n√∫t ƒë·ªÉ t·∫°o v√† hi·ªÉn th·ªã ph√¢n c·ª•m
                if st.button("Hi·ªÉn th·ªã d·ªØ li·ªáu v√† ph√¢n c·ª•m"):
                    # Hi·ªÉn th·ªã d·ªØ li·ªáu g·ªëc
                    st.write("**D·ªØ li·ªáu g·ªëc (3 c·ª•m):**")
                    for i in range(len(X)):
                        point = X[i]
                        cluster = original_label[i]
                        # st.write(f"ƒêi·ªÉm {point} thu·ªôc c·ª•m {cluster}")
                    
                    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m ban ƒë·∫ßu
                    fig = kmeans_display(X, original_label)  # L∆∞u figure t·ª´ h√†m
                    st.pyplot(fig)  # Hi·ªÉn th·ªã tr√™n Streamlit
                    
                    # Th·ª±c hi·ªán K-Means v√† hi·ªÉn th·ªã k·∫øt qu·∫£
                    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
                    kmeans.fit(X)
                    cluster_labels = kmeans.labels_
                    
                    st.write("**K·∫øt qu·∫£ ph√¢n c·ª•m K-Means:**")
                    for i in range(len(X)):
                        point = X[i]
                        cluster = cluster_labels[i]
                        # st.write(f"ƒêi·ªÉm {point} thu·ªôc c·ª•m {cluster}")
                    
                    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m K-Means
                    fig = kmeans_display(X, cluster_labels)  # L∆∞u figure t·ª´ h√†m
                    st.pyplot(fig)  # Hi·ªÉn th·ªã tr√™n Streamlit
                    
                    

                st.markdown("---")
                st.markdown("### üëç **∆Øu ƒëi·ªÉm**")
                st.write("""
                - ƒê∆°n gi·∫£n, d·ªÖ tri·ªÉn khai v√† t√≠nh to√°n nhanh v·ªõi d·ªØ li·ªáu nh·ªè ho·∫∑c trung b√¨nh.
                - Hi·ªáu qu·∫£ khi c√°c c·ª•m c√≥ h√¨nh d·∫°ng c·∫ßu (spherical) v√† k√≠ch th∆∞·ªõc t∆∞∆°ng ƒë∆∞∆°ng.
                """)

                st.markdown("### ‚ö†Ô∏è **Nh∆∞·ª£c ƒëi·ªÉm**")
                st.write("""
                - C·∫ßn ch·ªçn tr∆∞·ªõc s·ªë c·ª•m $K$, th∆∞·ªùng s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Elbow ho·∫∑c Silhouette ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng.
                - Nh·∫°y c·∫£m v·ªõi gi√° tr·ªã ban ƒë·∫ßu c·ªßa t√¢m c·ª•m, c√≥ th·ªÉ d·∫´n ƒë·∫øn k·∫øt qu·∫£ kh√°c nhau.
                - Kh√¥ng ho·∫°t ƒë·ªông t·ªët n·∫øu c·ª•m c√≥ h√¨nh d·∫°ng ph·ª©c t·∫°p (kh√¥ng ph·∫£i h√¨nh c·∫ßu) ho·∫∑c c√≥ k√≠ch th∆∞·ªõc, m·∫≠t ƒë·ªô kh√°c nhau.
                - Nh·∫°y c·∫£m v·ªõi nhi·ªÖu (outliers) v√† d·ªØ li·ªáu c√≥ thang ƒëo kh√°c nhau (y√™u c·∫ßu chu·∫©n h√≥a).
                """)

            elif model_option1 == "DBSCAN":
                st.markdown("## üîπ DBSCAN (Density-Based Clustering)")
                st.markdown("---")

                st.markdown("**Kh√°i ni·ªám**")
                st.write("""
                - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** l√† m·ªôt thu·∫≠t to√°n ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô, kh√¥ng y√™u c·∫ßu x√°c ƒë·ªãnh tr∆∞·ªõc s·ªë l∆∞·ª£ng c·ª•m.
                - Ph√π h·ª£p v·ªõi d·ªØ li·ªáu c√≥ h√¨nh d·∫°ng c·ª•m ph·ª©c t·∫°p v√† c√≥ kh·∫£ nƒÉng ph√°t hi·ªán nhi·ªÖu (outlier).
                """)
                st.markdown("---")
                st.markdown("### üîÑ **Quy tr√¨nh ho·∫°t ƒë·ªông c·ªßa DBSCAN**")
                st.write("""
                - **B∆∞·ªõc 1:** Ch·ªçn ng·∫´u nhi√™n m·ªôt ƒëi·ªÉm d·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c thƒÉm (unvisited).

                - **B∆∞·ªõc 2:** Ki·ªÉm tra s·ªë l∆∞·ª£ng ƒëi·ªÉm trong v√πng l√¢n c·∫≠n b√°n k√≠nh `eps` c·ªßa ƒëi·ªÉm ƒë√≥:
                    - N·∫øu s·ªë ƒëi·ªÉm >= `min_samples`, t·∫°o m·ªôt c·ª•m m·ªõi v√† th√™m ƒëi·ªÉm n√†y v√†o c·ª•m.
                    - N·∫øu kh√¥ng, ƒë√°nh d·∫•u ƒëi·ªÉm n√†y l√† nhi·ªÖu (noise).

                - **B∆∞·ªõc 3:** V·ªõi m·ªói ƒëi·ªÉm trong c·ª•m, ki·ªÉm tra l√¢n c·∫≠n c·ªßa n√≥:
                    - N·∫øu t√¨m th·∫•y c√°c ƒëi·ªÉm l√¢n c·∫≠n m·ªõi th·ªèa m√£n `min_samples`, th√™m ch√∫ng v√†o c·ª•m v√† ti·∫øp t·ª•c m·ªü r·ªông.
                    
                - **B∆∞·ªõc 4:** L·∫∑p l·∫°i qu√° tr√¨nh cho ƒë·∫øn khi t·∫•t c·∫£ c√°c ƒëi·ªÉm ƒë∆∞·ª£c thƒÉm ho·∫∑c ph√¢n lo·∫°i.

                - **B∆∞·ªõc 5:** K·∫øt th√∫c khi kh√¥ng c√≤n ƒëi·ªÉm n√†o ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω.
                """)
                st.markdown("---")
                st.markdown("### üìê **C√¥ng th·ª©c to√°n h·ªçc**")
                st.write("""
                DBSCAN s·ª≠ d·ª•ng kho·∫£ng c√°ch **Euclidean** ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm l√¢n c·∫≠n, ƒë∆∞·ª£c t√≠nh b·∫±ng c√¥ng th·ª©c:
                $$
                d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
                $$
                **trong ƒë√≥:**
                - $$d( p , q )$$ l√† hai ƒëi·ªÉm trong kh√¥ng gian \( n \) chi·ªÅu.
                - $$(p_i)$$ v√† $$(q_i)$$ v√† l√† t·ªça ƒë·ªô c·ªßa $$(p)$$ v√† $$(q)$$ ƒëi·ªÉm trong kh√¥ng gian n chi·ªÅu.
                """)
                st.markdown("---")
                st.markdown("### üîß **M·ªôt s·ªë c·∫£i ti·∫øn**")
                st.write("""
                - **OPTICS**: M·ªü r·ªông DBSCAN ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu c√≥ m·∫≠t ƒë·ªô thay ƒë·ªïi, t·∫°o ra th·ª© t·ª± ph√¢n c·∫•p c√°c c·ª•m.
                - **HDBSCAN**: K·∫øt h·ª£p ph√¢n c·ª•m ph√¢n c·∫•p v·ªõi DBSCAN, t·ª± ƒë·ªông ch·ªçn `eps` v√† c·∫£i thi·ªán hi·ªáu qu·∫£ tr√™n d·ªØ li·ªáu ph·ª©c t·∫°p.
                - **GDBSCAN**: T·ªïng qu√°t h√≥a DBSCAN ƒë·ªÉ √°p d·ª•ng cho c√°c lo·∫°i d·ªØ li·ªáu kh√¥ng gian kh√°c nhau.
                """)


                means = [[2, 2], [8, 3], [3, 6]]
                cov = [[1, 0], [0, 1]]
                N = 500

                # T·∫°o d·ªØ li·ªáu ph√¢n c·ª•m
                X0 = np.random.multivariate_normal(means[0], cov, N)
                X1 = np.random.multivariate_normal(means[1], cov, N)
                X2 = np.random.multivariate_normal(means[2], cov, N)

                X = np.concatenate((X0, X1, X2), axis=0)

                # T·∫°o nh√£n ban ƒë·∫ßu (d·ªØ li·ªáu g·ªëc)
                original_label = np.asarray([0]*N + [1]*N + [2]*N)

                # H√†m hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m, tr·∫£ v·ªÅ figure ƒë·ªÉ s·ª≠ d·ª•ng b√™n ngo√†i
                def dbscan_display(X, labels):
                    # X·ª≠ l√Ω nh√£n - DBSCAN c√≥ th·ªÉ g√°n nh√£n -1 cho c√°c ƒëi·ªÉm noise
                    unique_labels = np.unique(labels)
                    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Lo·∫°i b·ªè noise n·∫øu c√≥
                    
                    # T·∫°o figure v√† axes cho matplotlib
                    fig, ax = plt.subplots(figsize=(8, 6))
                    
                    # V·∫Ω t·ª´ng c·ª•m v·ªõi m√†u v√† ki·ªÉu kh√°c nhau
                    colors = ['blue', 'green', 'red', 'purple', 'orange', 'pink']  # M√†u cho c√°c c·ª•m
                    markers = ['^', 'o', 's', 'D', '*', 'v']  # Ki·ªÉu marker
                    
                    for label in unique_labels:
                        if label == -1:  # ƒêi·ªÉm noise
                            color = 'black'
                            marker = 'x'
                            label_text = 'Noise'
                        else:
                            color = colors[label % len(colors)]
                            marker = markers[label % len(markers)]
                            label_text = f'C·ª•m {label}'
                        
                        X_k = X[labels == label, :]
                        ax.plot(X_k[:, 0], X_k[:, 1], 
                                marker=marker, 
                                color=color, 
                                markersize=4, 
                                alpha=0.8, 
                                label=label_text)
                    
                    # Thi·∫øt l·∫≠p giao di·ªán bi·ªÉu ƒë·ªì
                    ax.set_xlabel("T·ªça ƒë·ªô X")
                    ax.set_ylabel("T·ªça ƒë·ªô Y")
                    ax.set_title("Bi·ªÉu ƒë·ªì ph√¢n c·ª•m DBSCAN v·ªõi d·ªØ li·ªáu c·ªë ƒë·ªãnh")
                    ax.legend()
                    ax.axis('equal')  # ƒê·∫£m b·∫£o t·ª∑ l·ªá tr·ª•c x v√† y b·∫±ng nhau
                    
                    # Hi·ªÉn th·ªã s·ªë c·ª•m (n·∫øu c√≥)
                    if n_clusters > 0:
                        st.write(f"S·ªë c·ª•m ƒë∆∞·ª£c ph√°t hi·ªán: {n_clusters}")
                    else:
                        st.write("Kh√¥ng ph√°t hi·ªán c·ª•m n√†o (c√≥ th·ªÉ t·∫•t c·∫£ l√† noise)")
                    
                    return fig  # Tr·∫£ v·ªÅ figure ƒë·ªÉ s·ª≠ d·ª•ng b√™n ngo√†i

                # Giao di·ªán Streamlit
                st.markdown("---")
                st.markdown("### üìä **Ph√¢n c·ª•m DBSCAN v·ªõi d·ªØ li·ªáu c·ªë ƒë·ªãnh**")

                # Th√™m c√°c tham s·ªë cho DBSCAN v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh ph√π h·ª£p h∆°n
                eps = st.slider("Kho·∫£ng c√°ch t·ªëi ƒëa (eps):", 0.1, 2.0, 0.5, 0.1)  # Gi·∫£m gi√° tr·ªã m·∫∑c ƒë·ªãnh t·ª´ 0.90 xu·ªëng 0.5
                min_samples = st.number_input("S·ªë ƒëi·ªÉm t·ªëi thi·ªÉu (min_samples):", min_value=1, max_value=50, value=10)  # TƒÉng gi√° tr·ªã m·∫∑c ƒë·ªãnh t·ª´ 5 l√™n 10

                # Hi·ªÉn th·ªã n√∫t ƒë·ªÉ t·∫°o v√† hi·ªÉn th·ªã ph√¢n c·ª•m
                if st.button("Hi·ªÉn th·ªã d·ªØ li·ªáu v√† ph√¢n c·ª•m"):
                    # Hi·ªÉn th·ªã d·ªØ li·ªáu g·ªëc
                    st.write("### D·ªØ li·ªáu g·ªëc (3 c·ª•m):")
                    for i in range(len(X)):
                        point = X[i]
                        cluster = original_label[i]
                        # st.write(f"ƒêi·ªÉm {point} thu·ªôc c·ª•m {cluster}")
                    
                    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m ban ƒë·∫ßu
                    fig = dbscan_display(X, original_label)  # Hi·ªÉn th·ªã d·ªØ li·ªáu g·ªëc
                    st.pyplot(fig)
                    
                    # Th·ª±c hi·ªán DBSCAN v√† hi·ªÉn th·ªã k·∫øt qu·∫£
                    dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X)
                    dbscan_labels = dbscan.labels_
                    
                    st.write("### K·∫øt qu·∫£ ph√¢n c·ª•m DBSCAN:")
                    for i in range(len(X)):
                        point = X[i]
                        cluster = dbscan_labels[i]
                        if cluster == -1:
                            cluster_text = "Noise"
                        else:
                            cluster_text = f"C·ª•m {cluster}"
                        # st.write(f"ƒêi·ªÉm {point} thu·ªôc {cluster_text}")
                    
                    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì ph√¢n c·ª•m DBSCAN
                    fig = dbscan_display(X, dbscan_labels)  # Hi·ªÉn th·ªã k·∫øt qu·∫£ DBSCAN
                    st.pyplot(fig)




                st.markdown("---")
                st.markdown("### üëç **∆Øu ƒëi·ªÉm**")
                st.write("""
                - Kh√¥ng c·∫ßn x√°c ƒë·ªãnh tr∆∞·ªõc s·ªë l∆∞·ª£ng c·ª•m.
                - Ph√°t hi·ªán t·ª± ƒë·ªông c√°c ƒëi·ªÉm nhi·ªÖu (outlier).
                - Hi·ªáu qu·∫£ v·ªõi c√°c c·ª•m c√≥ h√¨nh d·∫°ng b·∫•t k·ª≥ (kh√¥ng c·∫ßn gi·∫£ ƒë·ªãnh h√¨nh c·∫ßu nh∆∞ K-Means).
                """)

                st.markdown("### ‚ö†Ô∏è **Nh∆∞·ª£c ƒëi·ªÉm**")
                st.write("""
                - Nh·∫°y c·∫£m v·ªõi tham s·ªë `eps` v√† `min_samples`: Ch·ªçn sai c√≥ th·ªÉ d·∫´n ƒë·∫øn k·∫øt qu·∫£ kh√¥ng t·ªëi ∆∞u.
                - Hi·ªáu su·∫•t gi·∫£m khi m·∫≠t ƒë·ªô d·ªØ li·ªáu kh√¥ng ƒë·ªìng ƒë·ªÅu ho·∫∑c d·ªØ li·ªáu c√≥ chi·ªÅu cao (curse of dimensionality).
                - T·ªën k√©m v·ªÅ t√≠nh to√°n v·ªõi t·∫≠p d·ªØ li·ªáu l·ªõn (ƒë·ªô ph·ª©c t·∫°p \( O(n^2) \) n·∫øu kh√¥ng d√πng ch·ªâ m·ª•c kh√¥ng gian).
                """)




    with tab_info:
        with st.expander("**Th√¥ng tin d·ªØ li·ªáu**", expanded=True):
            st.markdown(
                '''
                **MNIST** l√† phi√™n b·∫£n ƒë∆∞·ª£c ch·ªânh s·ª≠a t·ª´ b·ªô d·ªØ li·ªáu **NIST g·ªëc** c·ªßa Vi·ªán Ti√™u chu·∫©n v√† C√¥ng ngh·ªá Qu·ªëc gia Hoa K·ª≥.  
                B·ªô d·ªØ li·ªáu ban ƒë·∫ßu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay t·ª´ **nh√¢n vi√™n b∆∞u ƒëi·ªán** v√† **h·ªçc sinh trung h·ªçc**.  

                C√°c nh√† nghi√™n c·ª©u **Yann LeCun, Corinna Cortes, v√† Christopher Burges** ƒë√£ x·ª≠ l√Ω, chu·∫©n h√≥a v√† chuy·ªÉn ƒë·ªïi b·ªô d·ªØ li·ªáu n√†y th√†nh **MNIST**  
                ƒë·ªÉ d·ªÖ d√†ng s·ª≠ d·ª•ng h∆°n cho c√°c b√†i to√°n nh·∫≠n d·∫°ng ch·ªØ s·ªë vi·∫øt tay.
                '''
            )
            # ƒê·∫∑c ƒëi·ªÉm c·ªßa b·ªô d·ªØ li·ªáu
        with st.expander("**ƒê·∫∑c ƒëi·ªÉm c·ªßa b·ªô d·ªØ li·ªáu**", expanded=True):
            st.markdown(
                '''
                - **S·ªë l∆∞·ª£ng ·∫£nh:** 70.000 ·∫£nh ch·ªØ s·ªë vi·∫øt tay  
                - **K√≠ch th∆∞·ªõc ·∫£nh:** M·ªói ·∫£nh c√≥ k√≠ch th∆∞·ªõc 28x28 pixel  
                - **C∆∞·ªùng ƒë·ªô ƒëi·ªÉm ·∫£nh:** T·ª´ 0 (m√†u ƒëen) ƒë·∫øn 255 (m√†u tr·∫Øng)  
                - **D·ªØ li·ªáu nh√£n:** M·ªói ·∫£nh ƒëi k√®m v·ªõi m·ªôt nh√£n s·ªë t·ª´ 0 ƒë·∫øn 9  
                '''
            )
            st.write(f"üîç S·ªë l∆∞·ª£ng ·∫£nh hu·∫•n luy·ªán: `{train_images.shape[0]}`")
            st.write(f"üîç S·ªë l∆∞·ª£ng ·∫£nh ki·ªÉm tra: `{test_images.shape[0]}`")


        with st.expander("**Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng m·∫´u c·ªßa t·ª´ng ch·ªØ s·ªë t·ª´ 0 ƒë·∫øn 9 trong t·∫≠p hu·∫•n luy·ªán**", expanded=True):
            label_counts = pd.Series(train_labels).value_counts().sort_index()
            # Hi·ªÉn th·ªã b·∫£ng d·ªØ li·ªáu d∆∞·ªõi bi·ªÉu ƒë·ªì
            df_counts = pd.DataFrame({"Ch·ªØ s·ªë": label_counts.index, "S·ªë l∆∞·ª£ng m·∫´u": label_counts.values})
            st.dataframe(df_counts)
            num_images = 10
            random_indices = random.sample(range(len(train_images)), num_images)
            fig, axes = plt.subplots(1, num_images, figsize=(15, 5))
            st.write("**M·ªôt s·ªë ·∫£nh v√≠ d·ª•:**")
            for ax, idx in zip(axes, random_indices):
                ax.imshow(train_images[idx], cmap='gray')
                ax.axis("off")
                ax.set_title(f"Label: {train_labels[idx]}")

            st.pyplot(fig)

        with st.expander("**Ki·ªÉm tra h√¨nh d·∫°ng c·ªßa t·∫≠p d·ªØ li·ªáu**", expanded=True):    
                # Ki·ªÉm tra h√¨nh d·∫°ng c·ªßa t·∫≠p d·ªØ li·ªáu
            st.write("üîç H√¨nh d·∫°ng t·∫≠p hu·∫•n luy·ªán:", train_images.shape)
            st.write("üîç H√¨nh d·∫°ng t·∫≠p ki·ªÉm tra:", test_images.shape)
            # Ki·ªÉm tra xem c√≥ gi√° tr·ªã pixel n√†o ngo√†i ph·∫°m vi 0-255 kh√¥ng
            if (train_images.min() < 0) or (train_images.max() > 255):
                st.error("‚ö†Ô∏è C·∫£nh b√°o: C√≥ gi√° tr·ªã pixel ngo√†i ph·∫°m vi 0-255!")
            else:
                st.success("‚úÖ D·ªØ li·ªáu pixel h·ª£p l·ªá (0 - 255).")

            # Chu·∫©n h√≥a d·ªØ li·ªáu
            train_images = train_images.astype("float32") / 255.0
            test_images = test_images.astype("float32") / 255.0

            # Hi·ªÉn th·ªã th√¥ng b√°o sau khi chu·∫©n h√≥a
            st.success("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v·ªÅ kho·∫£ng [0,1].")

            # Hi·ªÉn th·ªã b·∫£ng d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a (d·∫°ng s·ªë)
            num_samples = 5  # S·ªë l∆∞·ª£ng m·∫´u hi·ªÉn th·ªã
            df_normalized = pd.DataFrame(train_images[:num_samples].reshape(num_samples, -1))  

            st.write("**B·∫£ng d·ªØ li·ªáu sau khi chu·∫©n h√≥a**")
            st.dataframe(df_normalized)

            
            sample_size = 10_000  
            pixel_sample = np.random.choice(train_images.flatten(), sample_size, replace=False)

 

    with tab_load:
        with st.expander("**X·ª≠ l√Ω d·ªØ li·ªáu**", expanded=True):
            # Ki·ªÉm tra d·ªØ li·ªáu trong session_state
            if "train_images" in st.session_state and "train_labels" in st.session_state:
                # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh vector 1 chi·ªÅu
                try:
                    X = st.session_state.train_images.reshape(st.session_state.train_images.shape[0], -1)
                    y = st.session_state.train_labels  # Nh√£n (0-9)

                    # Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa X v√† y
                    if len(X) != len(y):
                        st.error("üö® K√≠ch th∆∞·ªõc c·ªßa d·ªØ li·ªáu v√† nh√£n kh√¥ng kh·ªõp. Vui l√≤ng ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o.")
                        st.stop()

                    # Ch·ªçn t·ª∑ l·ªá t·∫≠p Test (%)
                    test_size = st.slider("üîπ **Ch·ªçn t·ª∑ l·ªá t·∫≠p Test (%)**", min_value=10, max_value=90, value=20, step=5) / 100

                    # T√≠nh t·ª∑ l·ªá t·∫≠p Train t·ª± ƒë·ªông
                    train_size = 1.0 - test_size  # T·ª± ƒë·ªông t√≠nh t·ª∑ l·ªá train

                    # Chia t·∫≠p d·ªØ li·ªáu th√†nh Train v√† Test
                    X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(
                        X, y, test_size=test_size, random_state=42, stratify=y  # Th√™m stratify ƒë·ªÉ gi·ªØ t·ª∑ l·ªá nh√£n
                    )

                    # L∆∞u v√†o session_state
                    st.session_state.X_train = X_train_final
                    st.session_state.X_test = X_test_final
                    st.session_state.y_train = y_train_final
                    st.session_state.y_test = y_test_final

                    # T√≠nh t·ªïng s·ªë m·∫´u
                    total_samples = len(X)
                    train_samples = len(X_train_final)
                    test_samples = len(X_test_final)

                    # T√≠nh t·ª∑ l·ªá th·ª±c t·∫ø (%)
                    train_ratio = (train_samples / total_samples) * 100
                    test_ratio = (test_samples / total_samples) * 100

                    # Hi·ªÉn th·ªã th√¥ng b√°o v√† t·ª∑ l·ªá ph√¢n chia
                    st.write(f"üìä **T·ª∑ l·ªá ph√¢n chia**: Test={test_ratio:.0f}% , Train={train_ratio:.0f}%")
                    st.write("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† chia t√°ch.")
                    st.write(f"üîπ **K√≠ch th∆∞·ªõc t·∫≠p Train**: `{X_train_final.shape}`")
                    st.write(f"üîπ **K√≠ch th∆∞·ªõc t·∫≠p Test**: `{X_test_final.shape}`")
                except Exception as e:
                    st.error(f"üö® L·ªói khi x·ª≠ l√Ω d·ªØ li·ªáu: {str(e)}")
            else:
                st.error("üö® D·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c t·∫£i! Vui l√≤ng t·∫£i d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω.")
            


    with tab_preprocess:
        st.write("***Ph√¢n c·ª•m d·ªØ li·ªáu***")
        if "X_train" in st.session_state and "X_test" in st.session_state:
            # L·∫•y d·ªØ li·ªáu t·ª´ session_state (ch·ªâ d√πng X_train cho ph√¢n c·ª•m)
            X_train = st.session_state.X_train
            X_test = st.session_state.X_test

            # Chu·∫©n h√≥a d·ªØ li·ªáu
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # Gi·∫£m chi·ªÅu b·∫±ng PCA (2D) ƒë·ªÉ ph√¢n c·ª•m
            pca = PCA(n_components=2)
            X_train_pca = pca.fit_transform(X_train_scaled)

            st.session_state.scaler = scaler
            st.session_state.pca = pca
            st.session_state.X_train_pca = X_train_pca

            # Ch·ªçn ph∆∞∆°ng ph√°p ph√¢n c·ª•m
            clustering_method = st.selectbox("üîπ Ch·ªçn ph∆∞∆°ng ph√°p ph√¢n c·ª•m:", ["K-means", "DBSCAN"])

            if clustering_method == "K-means":
                k = st.slider("üî∏ S·ªë c·ª•m (K-means)", min_value=2, max_value=20, value=10)

                if st.button("üöÄ Ch·∫°y K-means"):
                    with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh..."):
                        with mlflow.start_run():
                            # ƒêo th·ªùi gian b·∫Øt ƒë·∫ßu
                            start_time = time.time()

                            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                            labels = kmeans.fit_predict(X_train_pca)

                            # ƒêo th·ªùi gian k·∫øt th√∫c v√† t√≠nh th·ªùi gian ph√¢n c·ª•m
                            end_time = time.time()
                            clustering_time = round(end_time - start_time, 2)  # L√†m tr√≤n 2 ch·ªØ s·ªë th·∫≠p ph√¢n

                            mlflow.log_param("algorithm", "K-means")
                            mlflow.log_param("k", k)
                            mlflow.log_param("max_iter", 300)

                            # T√≠nh Inertia
                            inertia = kmeans.inertia_
                            max_possible_inertia = np.sum(np.sum((X_train_pca - np.mean(X_train_pca, axis=0)) ** 2))
                            if max_possible_inertia > 0:
                                accuracy_percentage = (1 - (inertia / max_possible_inertia)) * 100
                            else:
                                accuracy_percentage = 0.0
                            accuracy_percentage = max(0.0, min(100.0, accuracy_percentage))
                            accuracy_percentage = round(accuracy_percentage, 2)

                            # T√≠nh c√°c th√¥ng tin b·ªï sung
                            num_samples = X_train_pca.shape[0]  # S·ªë m·∫´u ƒë√£ x·ª≠ l√Ω
                            num_clusters_actual = len(set(labels))  # S·ªë c·ª•m th·ª±c t·∫ø (trong tr∆∞·ªùng h·ª£p hi·∫øm, c√≥ th·ªÉ √≠t h∆°n k)

                            mlflow.log_metric("inertia", inertia)

                            # Hi·ªÉn th·ªã k·∫øt qu·∫£
                            with st.container(border=True):
                                st.write("### K·∫øt qu·∫£ ph√¢n c·ª•m v√† th√¥ng tin c·ªßa K-Means:")
                                st.write(f"**Ph∆∞∆°ng ph√°p ph√¢n c·ª•m ƒë√£ ch·ªçn:** K-means")
                                st.write(f"**S·ªë c·ª•m ƒë√£ ch·ªçn:** {k}")
                                st.write(f"**S·ªë c·ª•m th·ª±c t·∫ø:** {num_clusters_actual}")
                                st.write(f"**S·ªë m·∫´u ƒë√£ x·ª≠ l√Ω:** {num_samples}")
                                st.write(f"**Th·ªùi gian ph√¢n c·ª•m:** {clustering_time} gi√¢y")
                                st.write(f"**ƒê·ªô ch√≠nh x√°c c·ªßa ph√¢n c·ª•m K-Means:** {accuracy_percentage:.2f}%")

                            # Log m√¥ h√¨nh K-means
                            mlflow.sklearn.log_model(kmeans, "kmeans_model")
                            st.session_state.clustering_model = kmeans
                            st.session_state.clustering_method = "K-means"
                            st.session_state.labels = labels

                        mlflow.end_run()

            elif clustering_method == "DBSCAN":
                eps = st.slider("üî∏ B√°n k√≠nh v√πng l√¢n c·∫≠n (eps):", min_value=0.1, max_value=5.0, value=0.5, step=0.1)
                min_samples = st.slider("üî∏ S·ªë l∆∞·ª£ng ƒëi·ªÉm t·ªëi thi·ªÉu:", min_value=1, max_value=20, value=10)

                if st.button("üöÄ Ch·∫°y DBSCAN"):
                    with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh DBSCAN..."):
                        with mlflow.start_run():
                            # ƒêo th·ªùi gian b·∫Øt ƒë·∫ßu
                            start_time = time.time()

                            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                            labels = dbscan.fit_predict(X_train_pca)

                            # ƒêo th·ªùi gian k·∫øt th√∫c v√† t√≠nh th·ªùi gian ph√¢n c·ª•m
                            end_time = time.time()
                            clustering_time = round(end_time - start_time, 2)

                            mlflow.log_param("algorithm", "DBSCAN")
                            mlflow.log_param("eps", eps)
                            mlflow.log_param("min_samples", min_samples)

                            # T√≠nh s·ªë l∆∞·ª£ng c·ª•m (kh√¥ng t√≠nh noise, nh√£n -1)
                            num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                            # T√≠nh s·ªë l∆∞·ª£ng ƒëi·ªÉm nhi·ªÖu
                            noise_points = np.sum(labels == -1)
                            # T√≠nh s·ªë m·∫´u ƒë√£ x·ª≠ l√Ω
                            num_samples = X_train_pca.shape[0]

                            # Hi·ªÉn th·ªã k·∫øt qu·∫£
                            with st.container(border=True):
                                st.write("### K·∫øt qu·∫£ ph√¢n c·ª•m v√† th√¥ng tin c·ªßa DBSCAN:")
                                st.write(f"**Ph∆∞∆°ng ph√°p ph√¢n c·ª•m ƒë√£ ch·ªçn:** DBSCAN")
                                st.write(f"**S·ªë c·ª•m ƒë√£ ch·ªçn:** Kh√¥ng √°p d·ª•ng (t·ª± ƒë·ªông x√°c ƒë·ªãnh)")
                                st.write(f"**S·ªë c·ª•m th·ª±c t·∫ø:** {num_clusters}")
                                st.write(f"**S·ªë m·∫´u ƒë√£ x·ª≠ l√Ω:** {num_samples}")
                                st.write(f"**Th·ªùi gian ph√¢n c·ª•m:** {clustering_time} gi√¢y")
                                st.write(f"**S·ªë l∆∞·ª£ng ƒëi·ªÉm nhi·ªÖu (Noise Points):** {noise_points} ({round((noise_points / num_samples) * 100, 2)}%)")

                            # Log c√°c ch·ªâ s·ªë v√†o MLflow
                            mlflow.log_metric("num_clusters", num_clusters)
                            mlflow.log_metric("noise_points", noise_points)
                            for cluster, count in Counter(labels).items():
                                if cluster != -1:  # B·ªè qua nh√£n -1 (noise)
                                    mlflow.log_metric(f"cluster_{cluster}_size", count)

                            # Log m√¥ h√¨nh DBSCAN
                            mlflow.sklearn.log_model(dbscan, "dbscan_model")
                            st.session_state.clustering_model = dbscan
                            st.session_state.clustering_method = "DBSCAN"
                            st.session_state.labels = labels

                        mlflow.end_run()
        else:
            st.error("üö® D·ªØ li·ªáu ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω! H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ ch·∫°y ph·∫ßn ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc khi th·ª±c hi·ªán ph√¢n c·ª•m.")

    with tab_demo:
        with st.expander("**D·ª± ƒëo√°n c·ª•m cho ·∫£nh**", expanded=False):
            st.write("T·∫£i l√™n ·∫£nh ch·ªØ s·ªë vi·∫øt tay (28x28 pixel, grayscale) ƒë·ªÉ d·ª± ƒëo√°n c·ª•m:")

            # Ki·ªÉm tra v√† hi·ªÉn th·ªã th√¥ng tin ph√¢n c·ª•m ƒë√£ hu·∫•n luy·ªán
            if "clustering_method" in st.session_state and "clustering_model" in st.session_state:
                clustering_method = st.session_state.clustering_method
                model = st.session_state.clustering_model
                num_samples = st.session_state.X_train_pca.shape[0] if "X_train_pca" in st.session_state else 0
                clustering_time = st.session_state.get("clustering_time", "Kh√¥ng c√≥ d·ªØ li·ªáu")

                with st.container(border=True):
                    st.write("### Th√¥ng tin ph√¢n c·ª•m ƒë√£ hu·∫•n luy·ªán:")
                    st.write(f"**Ph∆∞∆°ng ph√°p ph√¢n c·ª•m:** {clustering_method}")
                    if clustering_method == "K-means":
                        k = model.n_clusters  # L·∫•y s·ªë c·ª•m t·ª´ m√¥ h√¨nh K-means
                        accuracy_percentage = st.session_state.get("accuracy_percentage", "Kh√¥ng c√≥ d·ªØ li·ªáu")
                        num_clusters_actual = len(set(st.session_state.labels)) if "labels" in st.session_state else k
                        st.write(f"**S·ªë c·ª•m ƒë√£ ch·ªçn:** {k}")
                        st.write(f"**S·ªë c·ª•m th·ª±c t·∫ø:** {num_clusters_actual}")
                        st.write(f"**S·ªë m·∫´u ƒë√£ x·ª≠ l√Ω:** {num_samples}")
                        # st.write(f"**Th·ªùi gian ph√¢n c·ª•m:** {clustering_time} gi√¢y")
                        # st.write(f"**ƒê·ªô ch√≠nh x√°c c·ªßa ph√¢n c·ª•m K-Means:** {accuracy_percentage}")
                    elif clustering_method == "DBSCAN":
                        labels = st.session_state.labels if "labels" in st.session_state else []
                        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                        noise_points = np.sum(labels == -1) if labels.size > 0 else 0
                        st.write(f"**S·ªë c·ª•m th·ª±c t·∫ø:** {num_clusters}")
                        st.write(f"**S·ªë m·∫´u ƒë√£ x·ª≠ l√Ω:** {num_samples}")
                        st.write(f"**S·ªë l∆∞·ª£ng ƒëi·ªÉm nhi·ªÖu (Noise Points):** {noise_points} ({round((noise_points / num_samples) * 100, 2)}%) ")

                # T·∫£i file ·∫£nh
                uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh (.png, .jpg, .jpeg)", type=["png", "jpg", "jpeg"], key="upload_predict")

                # D·ª± ƒëo√°n n·∫øu c√≥ ·∫£nh ƒë∆∞·ª£c t·∫£i l√™n
                if uploaded_file is not None:
                    # ƒê·ªçc v√† x·ª≠ l√Ω ·∫£nh
                    image = Image.open(uploaded_file).convert('L')  # Chuy·ªÉn th√†nh grayscale
                    image = image.resize((28, 28))  # ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc 28x28
                    image_array = np.array(image) / 255.0  # Chu·∫©n h√≥a v·ªÅ [0, 1]
                    image_vector = image_array.reshape(1, -1)  # Chuy·ªÉn th√†nh vector 1 chi·ªÅu (784 chi·ªÅu)

                    # Chu·∫©n h√≥a ·∫£nh m·ªõi b·∫±ng scaler ƒë√£ fit tr√™n X_train
                    if "scaler" in st.session_state:
                        image_scaled = st.session_state.scaler.transform(image_vector)
                    else:
                        st.error("üö® Scaler ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng ch·∫°y ph√¢n c·ª•m trong tab 'Ph√¢n c·ª•m d·ªØ li·ªáu' tr∆∞·ªõc.")
                        st.stop()

                    # Gi·∫£m chi·ªÅu ·∫£nh m·ªõi b·∫±ng PCA ƒë√£ fit tr√™n X_train
                    if "pca" in st.session_state:
                        image_pca = st.session_state.pca.transform(image_scaled)
                    else:
                        st.error("üö® PCA ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng ch·∫°y ph√¢n c·ª•m trong tab 'Ph√¢n c·ª•m d·ªØ li·ªáu' tr∆∞·ªõc.")
                        st.stop()

                    # Ki·ªÉm tra xem m√¥ h√¨nh ph√¢n c·ª•m ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán ch∆∞a
                    if "clustering_model" not in st.session_state or "clustering_method" not in st.session_state:
                        st.error("üö® M√¥ h√¨nh ph√¢n c·ª•m ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Vui l√≤ng ch·∫°y ph√¢n c·ª•m trong tab 'Ph√¢n c·ª•m d·ªØ li·ªáu' tr∆∞·ªõc.")
                    else:
                        clustering_method = st.session_state.clustering_method
                        model = st.session_state.clustering_model

                        if clustering_method == "K-means":
                            # D·ª± ƒëo√°n c·ª•m cho ·∫£nh m·ªõi d·ª±a tr√™n kho·∫£ng c√°ch ƒë·∫øn t√¢m c·ª•m
                            try:
                                cluster_label = model.predict(image_pca)[0]
                                with st.container(border=True):
                                    st.write("### K·∫øt qu·∫£ d·ª± ƒëo√°n cho ·∫£nh m·ªõi:")
                                    st.write(f"·∫¢nh c·ªßa b·∫°n thu·ªôc **C·ª•m {cluster_label}**")
                                    st.image(image, caption="·∫¢nh ƒë∆∞·ª£c t·∫£i l√™n", width=100)
                            except Exception as e:
                                st.error(f"üö® L·ªói khi d·ª± ƒëo√°n v·ªõi K-means: {str(e)}")

                        elif clustering_method == "DBSCAN":
                            # V·ªõi DBSCAN, d·ª± ƒëo√°n d·ª±a tr√™n kho·∫£ng c√°ch ƒë·∫øn c√°c ƒëi·ªÉm ƒë√£ ph√¢n c·ª•m
                            if "X_train_pca" in st.session_state and "labels" in st.session_state:
                                X_train_pca = st.session_state.X_train_pca
                                labels = st.session_state.labels

                                distances = np.linalg.norm(X_train_pca - image_pca, axis=1)
                                nearest_point_idx = np.argmin(distances)
                                nearest_label = labels[nearest_point_idx]

                                with st.container(border=True):
                                    st.write("### K·∫øt qu·∫£ d·ª± ƒëo√°n cho ·∫£nh m·ªõi:")
                                    if nearest_label == -1:
                                        st.write("·∫¢nh c·ªßa b·∫°n ƒë∆∞·ª£c coi l√† **ƒëi·ªÉm nhi·ªÖu (Noise)**")
                                    else:
                                        st.write(f"·∫¢nh c·ªßa b·∫°n thu·ªôc **C·ª•m {nearest_label}**")
                                    st.image(image, caption="·∫¢nh ƒë∆∞·ª£c t·∫£i l√™n", width=100)
                            else:
                                st.error("üö® D·ªØ li·ªáu hu·∫•n luy·ªán PCA ho·∫∑c nh√£n ch∆∞a ƒë∆∞·ª£c l∆∞u. Vui l√≤ng ch·∫°y ph√¢n c·ª•m trong tab 'Ph√¢n c·ª•m d·ªØ li·ªáu' tr∆∞·ªõc.")
                                st.stop()



    with tab_mlflow:
        st.header("Th√¥ng tin Hu·∫•n luy·ªán & MLflow UI")
        try:
            client = MlflowClient()
            experiment_name = "Clustering"

            # Ki·ªÉm tra n·∫øu experiment ƒë√£ t·ªìn t·∫°i
            experiment = client.get_experiment_by_name(experiment_name)
            if experiment is None:
                experiment_id = client.create_experiment(experiment_name)
                st.success(f"Experiment m·ªõi ƒë∆∞·ª£c t·∫°o v·ªõi ID: {experiment_id}")
            else:
                experiment_id = experiment.experiment_id
                st.info(f"ƒêang s·ª≠ d·ª•ng experiment ID: {experiment_id}")

            mlflow.set_experiment(experiment_name)

            # Truy v·∫•n c√°c run trong experiment
            runs = client.search_runs(experiment_ids=[experiment_id])

            # 1) Ch·ªçn v√† ƒë·ªïi t√™n Run Name
            st.subheader("ƒê·ªïi t√™n Run")
            if runs:
                run_options = {run.info.run_id: f"{run.data.tags.get('mlflow.runName', 'Unnamed')} - {run.info.run_id}"
                            for run in runs}
                selected_run_id_for_rename = st.selectbox("Ch·ªçn Run ƒë·ªÉ ƒë·ªïi t√™n:", 
                                                        options=list(run_options.keys()), 
                                                        format_func=lambda x: run_options[x])
                new_run_name = st.text_input("Nh·∫≠p t√™n m·ªõi cho Run:", 
                                            value=run_options[selected_run_id_for_rename].split(" - ")[0])
                if st.button("C·∫≠p nh·∫≠t t√™n Run"):
                    if new_run_name.strip():
                        client.set_tag(selected_run_id_for_rename, "mlflow.runName", new_run_name.strip())
                        st.success(f"ƒê√£ c·∫≠p nh·∫≠t t√™n Run th√†nh: {new_run_name.strip()}")
                    else:
                        st.warning("Vui l√≤ng nh·∫≠p t√™n m·ªõi cho Run.")
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë∆∞·ª£c log.")

            # 2) X√≥a Run
            st.subheader("Danh s√°ch Run")
            if runs:
                selected_run_id_to_delete = st.selectbox("", 
                                                        options=list(run_options.keys()), 
                                                        format_func=lambda x: run_options[x])
                if st.button("X√≥a Run", key="delete_run"):
                    client.delete_run(selected_run_id_to_delete)
                    st.success(f"ƒê√£ x√≥a Run {run_options[selected_run_id_to_delete]} th√†nh c√¥ng!")
                    st.experimental_rerun()  # T·ª± ƒë·ªông l√†m m·ªõi giao di·ªán
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë·ªÉ x√≥a.")

            # 3) Danh s√°ch c√°c th√≠ nghi·ªám
            st.subheader("Danh s√°ch c√°c Run ƒë√£ log")
            if runs:
                selected_run_id = st.selectbox("Ch·ªçn Run ƒë·ªÉ xem chi ti·∫øt:", 
                                            options=list(run_options.keys()), 
                                            format_func=lambda x: run_options[x])

                # 4) Hi·ªÉn th·ªã th√¥ng tin chi ti·∫øt c·ªßa Run ƒë∆∞·ª£c ch·ªçn
                selected_run = client.get_run(selected_run_id)
                st.write(f"**Run ID:** {selected_run_id}")
                st.write(f"**Run Name:** {selected_run.data.tags.get('mlflow.runName', 'Unnamed')}")

                st.markdown("### Tham s·ªë ƒë√£ log")
                st.json(selected_run.data.params)

                st.markdown("### Ch·ªâ s·ªë ƒë√£ log")
                metrics = {
                    "Algorithm": selected_run.data.params.get("algorithm", "N/A"),
                    "K (for K-means)": selected_run.data.params.get("k", "N/A"),
                    "Max Iter (for K-means)": selected_run.data.params.get("max_iter", "N/A"),
                    "EPS (for DBSCAN)": selected_run.data.params.get("eps", "N/A"),
                    "Min Samples (for DBSCAN)": selected_run.data.params.get("min_samples", "N/A"),
                    "Inertia (for K-means)": selected_run.data.metrics.get("inertia", "N/A"),
                    "Num Clusters (for DBSCAN)": selected_run.data.metrics.get("num_clusters", "N/A"),
                    "Noise Points (for DBSCAN)": selected_run.data.metrics.get("noise_points", "N/A")
                }
                st.json(metrics)

                # 5) N√∫t b·∫•m m·ªü MLflow UI
                st.subheader("Truy c·∫≠p MLflow UI")
                mlflow_url = "https://dagshub.com/Dung2204/HMVPython.mlflow"
                if st.button("M·ªü MLflow UI"):
                    st.markdown(f'**[Click ƒë·ªÉ m·ªü MLflow UI]({mlflow_url})**')
            else:
                st.info("Ch∆∞a c√≥ Run n√†o ƒë∆∞·ª£c log. Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc.")

        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi v·ªõi MLflow: {e}")

if __name__ == "__main__":
    run_ClusteringMinst_app()    


# st.write(f"MLflow Tracking URI: {mlflow.get_tracking_uri()}")
# with st.expander("üñºÔ∏è ƒê√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh ph√¢n c·ª•m", expanded=True):
#     # st.write(f"MLflow Tracking URI: {mlflow.get_tracking_uri()}")
#     print("üéØ Ki·ªÉm tra tr√™n DagsHub: https://dagshub.com/Dung2204/Minst-mlflow.mlflow")


# # # # cd "C:\Users\Dell\OneDrive\Pictures\Documents\Code\python\OpenCV\HMVPYTHON\BaiThucHanh4"




